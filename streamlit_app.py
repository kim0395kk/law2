# streamlit_app.py
# -*- coding: utf-8 -*-
"""
Law AI Search (Streamlit) - law.go.kr DRF Open API ê¸°ë°˜
- lawSearch.do (ëª©ë¡/ê²€ìƒ‰) + lawService.do (ë³¸ë¬¸)
- "AI ì„œì¹˜" = (ìƒí™© í…ìŠ¤íŠ¸ -> í‚¤ì›Œë“œ í™•ì¥) -> ë³¸ë¬¸ê²€ìƒ‰(search=2) + ë²•ë ¹ëª…ê²€ìƒ‰(search=1) -> ë””ë“€í”„ -> ë³¸ë¬¸ì—ì„œ ì¡°ë¬¸ í•˜ì´ë¼ì´íŠ¸
"""

import json
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import requests
import streamlit as st


LAW_SEARCH_URL = "https://www.law.go.kr/DRF/lawSearch.do"
LAW_SERVICE_URL = "https://www.law.go.kr/DRF/lawService.do"

DEFAULT_TIMEOUT = 12


# -----------------------------
# Utilities
# -----------------------------
def _safe_int(x: Any) -> Optional[int]:
    try:
        return int(str(x).strip())
    except Exception:
        return None


def _first_key(d: Dict[str, Any], keys: List[str]) -> Optional[str]:
    lower_map = {k.lower(): k for k in d.keys()}
    for want in keys:
        if want.lower() in lower_map:
            return lower_map[want.lower()]
    return None


def _normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()


def _highlight(text: str, terms: List[str]) -> str:
    if not text:
        return text
    out = text
    for t in sorted(set([x for x in terms if x]), key=len, reverse=True):
        # ë„ˆë¬´ ì§§ì€ í† í°ì€ ì œì™¸(ë…¸ì´ì¦ˆ ë°©ì§€)
        if len(t) < 2:
            continue
        out = re.sub(re.escape(t), lambda m: f"**{m.group(0)}**", out, flags=re.IGNORECASE)
    return out


def _extract_terms_from_situation(situation: str, max_terms: int = 8) -> List[str]:
    """
    LLM ì—†ì´ë„ ëŒì•„ê°€ê²Œ 'ê°„ë‹¨ í‚¤ì›Œë“œ ì¶”ì¶œ' (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ íœ´ë¦¬ìŠ¤í‹±)
    - ìˆ«ì/ê¸°í˜¸ ì œê±° í›„ 2~15ì í† í°
    - í”í•œ ë¶ˆìš©ì–´ ì œê±°
    """
    if not situation:
        return []

    stop = set([
        "ìˆìŠµë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë˜ì—ˆìŠµë‹ˆë‹¤", "ëŒ€í•´ì„œ", "ê´€ë ¨", "ê²€í† ", "ìš”ì²­", "ë¬¸ì˜",
        "ë¯¼ì›", "ì²˜ë¦¬", "ê°€ëŠ¥", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ë•Œë¬¸", "ê²½ìš°", "ê·¸ë¦¬ê³ ", "ë˜í•œ",
        "ì €í¬", "ìš°ë¦¬", "ê·€í•˜", "ì‚¬í•­", "ë¶€ë¶„", "ëŒ€í•œ", "í•´ì„œ", "ì…ë‹ˆë‹¤"
    ])

    s = re.sub(r"[^\w\sê°€-í£]", " ", situation)
    s = _normalize_space(s)
    tokens = [t for t in s.split(" ") if 2 <= len(t) <= 15]
    tokens = [t for t in tokens if t not in stop]
    # ë¹ˆë„ ê¸°ë°˜ ìƒìœ„
    freq: Dict[str, int] = {}
    for t in tokens:
        freq[t] = freq.get(t, 0) + 1
    ranked = sorted(freq.items(), key=lambda kv: (-kv[1], -len(kv[0])))
    return [k for k, _ in ranked[:max_terms]]


def _dedupe_by(items: List[Dict[str, Any]], key_candidates: List[str]) -> List[Dict[str, Any]]:
    seen = set()
    out = []
    for it in items:
        k = None
        for kc in key_candidates:
            if kc in it and it.get(kc) not in (None, "", "0"):
                k = str(it.get(kc))
                break
        if not k:
            k = json.dumps(it, ensure_ascii=False, sort_keys=True)[:120]
        if k in seen:
            continue
        seen.add(k)
        out.append(it)
    return out


# -----------------------------
# Law.go.kr API Client
# -----------------------------
@dataclass
class LawGoClient:
    oc: str

    def _get_json(self, url: str, params: Dict[str, Any]) -> Dict[str, Any]:
        r = requests.get(url, params=params, timeout=DEFAULT_TIMEOUT)
        r.raise_for_status()
        # law.go.krì€ JSONì´ë”ë¼ë„ content-typeì´ ì• ë§¤í•œ ê²½ìš°ê°€ ìˆì–´ try/except
        try:
            return r.json()
        except Exception:
            # fallback: textë¥¼ json.loads
            return json.loads(r.text)

    def search_laws(
        self,
        query: str,
        search_scope: int = 1,  # 1: ë²•ë ¹ëª…, 2: ë³¸ë¬¸ê²€ìƒ‰
        page: int = 1,
        display: int = 20,
        sort: str = "efdes",  # ì‹œí–‰ì¼ì ë‚´ë¦¼ì°¨ìˆœ
    ) -> Dict[str, Any]:
        params = {
            "OC": self.oc,
            "target": "law",
            "type": "JSON",
            "query": query,
            "search": search_scope,
            "page": page,
            "display": display,
            "sort": sort,
        }
        return self._get_json(LAW_SEARCH_URL, params)

    def get_law_detail(
        self,
        law_id: Optional[str] = None,
        mst: Optional[str] = None,
        jo: Optional[str] = None,  # 6ìë¦¬ ì¡°ë²ˆí˜¸(ì˜µì…˜)
        lang: str = "KO",
    ) -> Dict[str, Any]:
        params = {
            "OC": self.oc,
            "target": "law",
            "type": "JSON",
            "LANG": lang,
        }
        # ID ë˜ëŠ” MST ì¤‘ í•˜ë‚˜ í•„ìˆ˜ :contentReference[oaicite:2]{index=2}
        if law_id:
            params["ID"] = law_id
        if mst:
            params["MST"] = mst
        if jo:
            params["JO"] = jo

        return self._get_json(LAW_SERVICE_URL, params)


# -----------------------------
# Parsers (êµ¬ì¡°ê°€ ì¡°ê¸ˆ ë‹¬ë¼ë„ ìµœëŒ€í•œ ê²¬ë”¤)
# -----------------------------
def parse_search_results(payload: Dict[str, Any]) -> Tuple[int, List[Dict[str, Any]]]:
    """
    lawSearch ê²°ê³¼ì—ì„œ totalCnt + law ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
    ê°€ì´ë“œì˜ ì‘ë‹µ í•„ë“œ: totalCnt, law(ë°˜ë³µ) ë“± :contentReference[oaicite:3]{index=3}
    """
    if not isinstance(payload, dict):
        return 0, []

    root = payload
    # ë³´í†µ ìµœìƒìœ„ì— LawSearch ë˜ëŠ” searchResult ê°™ì€ í‚¤ê°€ í•œ ë²ˆ ê°ì‹¸ëŠ” ê²½ìš°ê°€ ìˆìŒ
    if len(root) == 1 and isinstance(next(iter(root.values())), dict):
        root = next(iter(root.values()))

    total = 0
    for k in ["totalCnt", "TotalCnt", "total_count"]:
        if k in root:
            total = _safe_int(root.get(k)) or 0
            break

    # law ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    law_key = _first_key(root, ["law", "Law", "laws"])
    laws = root.get(law_key, []) if law_key else []
    if isinstance(laws, dict):
        laws = [laws]
    if not isinstance(laws, list):
        laws = []

    # í‘œì¤€í™”(í•„ë“œëª…ì´ í•œê¸€ì¸ ê²½ìš°ë„ ìˆì–´ì„œ "ìˆëŠ” ê·¸ëŒ€ë¡œ" ìœ ì§€ + ìì£¼ ì“°ëŠ” í‚¤ë§Œ ë§¤í•‘)
    norm: List[Dict[str, Any]] = []
    for it in laws:
        if not isinstance(it, dict):
            continue
        norm.append(it)
    return total, norm


def extract_articles_from_detail(payload: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    lawService ìƒì„¸(JSON)ì—ì„œ 'ì¡°ë¬¸ë‚´ìš©'ì´ ìˆëŠ” ê°ì²´ë“¤ì„ ì „ë¶€ ìˆ˜ì§‘
    ê°€ì´ë“œ ì‘ë‹µ í•„ë“œì— 'ì¡°ë¬¸ë‚´ìš©/ì¡°ë¬¸ë²ˆí˜¸/ì¡°ë¬¸ì œëª©' ë“±ì´ ìˆìŒ :contentReference[oaicite:4]{index=4}
    """
    articles: List[Dict[str, Any]] = []

    def walk(x: Any):
        if isinstance(x, dict):
            if "ì¡°ë¬¸ë‚´ìš©" in x:
                articles.append(x)
            for v in x.values():
                walk(v)
        elif isinstance(x, list):
            for v in x:
                walk(v)

    walk(payload)
    return articles


def pick_law_meta(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìƒì„¸ ê²°ê³¼ì—ì„œ ë©”íƒ€(ë²•ë ¹ëª…/ì‹œí–‰ì¼/ì†Œê´€ë¶€ì²˜ ë“±) í›„ë³´ í‚¤ë¥¼ ëŒ€ì¶© ê¸ì–´ì˜´.
    """
    meta = {}

    def find_first(d: Dict[str, Any], candidates: List[str]) -> Optional[Any]:
        for c in candidates:
            if c in d and d[c] not in (None, "", "0"):
                return d[c]
        return None

    # ì „ì²´ë¥¼ í›‘ì–´ ê°€ì¥ ë¨¼ì € ë§Œë‚˜ëŠ” ë©”íƒ€ í‚¤ë“¤ ìˆ˜ì§‘
    def walk(d: Any):
        if not isinstance(d, dict):
            return
        for key, val in d.items():
            if key in ["ë²•ë ¹ëª…_í•œê¸€", "ë²•ë ¹ëª…í•œê¸€", "ë²•ë ¹ëª…", "ë²•ë ¹ëª…ì•½ì¹­", "ì†Œê´€ë¶€ì²˜", "ì†Œê´€ë¶€ì²˜ëª…", "ì‹œí–‰ì¼ì", "ê³µí¬ì¼ì", "ê³µí¬ë²ˆí˜¸", "ë²•ë ¹ID"]:
                if key not in meta and val not in (None, "", "0"):
                    meta[key] = val
        for v in d.values():
            if isinstance(v, dict):
                walk(v)

    walk(payload)

    # ë³´ê¸° ì¢‹ì€ ë³„ì¹­
    meta_view = {
        "ë²•ë ¹ëª…": find_first(meta, ["ë²•ë ¹ëª…_í•œê¸€", "ë²•ë ¹ëª…í•œê¸€", "ë²•ë ¹ëª…"]),
        "ì‹œí–‰ì¼ì": find_first(meta, ["ì‹œí–‰ì¼ì"]),
        "ê³µí¬ì¼ì": find_first(meta, ["ê³µí¬ì¼ì"]),
        "ê³µí¬ë²ˆí˜¸": find_first(meta, ["ê³µí¬ë²ˆí˜¸"]),
        "ì†Œê´€ë¶€ì²˜": find_first(meta, ["ì†Œê´€ë¶€ì²˜", "ì†Œê´€ë¶€ì²˜ëª…"]),
        "ë²•ë ¹ID": find_first(meta, ["ë²•ë ¹ID"]),
    }
    return {k: v for k, v in meta_view.items() if v not in (None, "", "0")}


# -----------------------------
# Streamlit App
# -----------------------------
st.set_page_config(page_title="ë²•ë ¹ AI ì„œì¹˜ (law.go.kr API)", page_icon="âš–ï¸", layout="wide")

st.title("âš–ï¸ ë²•ë ¹ AI ì„œì¹˜ (êµ­ê°€ë²•ë ¹ì •ë³´ Open API)")
st.caption("â€» ë¯¼ê°ì •ë³´(ì„±ëª…/ì£¼ì†Œ/ì—°ë½ì²˜/ì°¨ëŸ‰ë²ˆí˜¸ ë“±) ì…ë ¥ ê¸ˆì§€. ê²°ê³¼ëŠ” ì°¸ê³ ìš© ì´ˆì•ˆì…ë‹ˆë‹¤.")

# Sidebar: Secrets
with st.sidebar:
    st.header("ì„¤ì •")
    oc = st.secrets.get("LAWGO_OC", "") if hasattr(st, "secrets") else ""
    oc = st.text_input("LAWGO_OC (ë²•ë ¹ Open API OC)", value=oc, type="password", help="law.go.kr Open API ì¸ì¦ê°’(OC).")
    if not oc:
        st.warning("OCê°€ ë¹„ì–´ìˆìœ¼ë©´ ê²€ìƒ‰ì´ ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    mode = st.radio(
        "ê²€ìƒ‰ ëª¨ë“œ",
        ["AI ì„œì¹˜(ìƒí™©â†’í‚¤ì›Œë“œ í™•ì¥)", "í‚¤ì›Œë“œ ì§ì ‘ê²€ìƒ‰"],
        index=0,
    )

    search_scope = st.selectbox(
        "ê²€ìƒ‰ ë²”ìœ„",
        options=[("ë²•ë ¹ëª…(ë¹ ë¦„)", 1), ("ë³¸ë¬¸ê²€ìƒ‰(ê°•í•¨)", 2)],
        index=1,
        format_func=lambda x: x[0],
    )[1]

    display = st.slider("ê²°ê³¼ ê°œìˆ˜", 10, 100, 30, 10)
    sort = st.selectbox(
        "ì •ë ¬",
        options=[
            ("ì‹œí–‰ì¼ ë‚´ë¦¼ì°¨ìˆœ(ì¶”ì²œ)", "efdes"),
            ("ì‹œí–‰ì¼ ì˜¤ë¦„ì°¨ìˆœ", "efasc"),
            ("ê³µí¬ì¼ ë‚´ë¦¼ì°¨ìˆœ", "ddes"),
            ("ê³µí¬ì¼ ì˜¤ë¦„ì°¨ìˆœ", "dasc"),
            ("ë²•ë ¹ëª… ì˜¤ë¦„ì°¨ìˆœ", "lasc"),
            ("ë²•ë ¹ëª… ë‚´ë¦¼ì°¨ìˆœ", "ldes"),
        ],
        index=0,
    )[1]

    advanced = st.checkbox("ê³ ê¸‰: ì›ë³¸ JSON ë³´ê¸°(ë””ë²„ê·¸)", value=False)


if not oc:
    st.stop()

client = LawGoClient(oc=oc)


@st.cache_data(ttl=3600, show_spinner=False)
def cached_search(query: str, search_scope: int, page: int, display: int, sort: str) -> Dict[str, Any]:
    return client.search_laws(query=query, search_scope=search_scope, page=page, display=display, sort=sort)


@st.cache_data(ttl=3600, show_spinner=False)
def cached_detail(law_id: Optional[str], mst: Optional[str]) -> Dict[str, Any]:
    return client.get_law_detail(law_id=law_id, mst=mst, lang="KO")


# Input
colL, colR = st.columns([1.2, 1])
with colL:
    if mode.startswith("AI"):
        situation = st.text_area(
            "ìƒí™©/ì§ˆë¬¸ì„ ë„£ìœ¼ë©´, í‚¤ì›Œë“œë¥¼ ë½‘ì•„ì„œ ë²•ë ¹ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.",
            height=140,
            placeholder="ì˜ˆ: ë¬´ë‹¨ë°©ì¹˜ ì°¨ëŸ‰ ê°•ì œì²˜ë¦¬ ì ˆì°¨ì™€ ê·¼ê±°ì¡°ë¬¸, í†µì§€/ê³µì‹œì†¡ë‹¬ ë°©ì‹ê¹Œì§€ ì •ë¦¬ í•„ìš”",
        )
        manual_terms = st.text_input("ì¶”ê°€ í‚¤ì›Œë“œ(ì„ íƒ, ì‰¼í‘œë¡œ êµ¬ë¶„)", value="")
    else:
        situation = ""
        query_direct = st.text_input("ê²€ìƒ‰ì–´(ë²•ë ¹ëª…/í‚¤ì›Œë“œ)", value="", placeholder="ì˜ˆ: ìë™ì°¨ê´€ë¦¬ë²•, ë¬´ë‹¨ë°©ì¹˜, ê³µì‹œì†¡ë‹¬")

with colR:
    st.markdown("### ì‚¬ìš© íŒ")
    st.markdown(
        "- **ë³¸ë¬¸ê²€ìƒ‰(ê°•í•¨)**ì€ ê²°ê³¼ê°€ í’ë¶€í•˜ì§€ë§Œ ëŠë¦´ ìˆ˜ ìˆì–´ìš”.\n"
        "- ê²°ê³¼ì—ì„œ ë²•ë ¹ì„ í´ë¦­í•˜ë©´ **ì¡°ë¬¸ ë‹¨ìœ„ë¡œ í•˜ì´ë¼ì´íŠ¸**í•©ë‹ˆë‹¤.\n"
        "- ë²•ë ¹ ìƒì„¸ëŠ” `lawService.do?target=law`ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤. :contentReference[oaicite:5]{index=5}"
    )

run = st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True)

if run:
    try:
        if mode.startswith("AI"):
            base_terms = _extract_terms_from_situation(situation, max_terms=8)
            extra = [t.strip() for t in manual_terms.split(",") if t.strip()]
            terms = _dedupe_by([{"t": x} for x in (base_terms + extra)], ["t"])
            terms = [x["t"] for x in terms][:10]

            if not terms:
                st.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ê±°ë‚˜, ì¶”ê°€ í‚¤ì›Œë“œë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
                st.stop()

            st.write("**ì¶”ì¶œ/ì‚¬ìš© í‚¤ì›Œë“œ:** ", ", ".join(terms))

            # í‚¤ì›Œë“œë³„ ê²€ìƒ‰(í˜ì´ì§€ 1 ê³ ì •)
            all_items: List[Dict[str, Any]] = []
            for t in terms:
                payload = cached_search(query=t, search_scope=search_scope, page=1, display=display, sort=sort)
                _, items = parse_search_results(payload)
                for it in items:
                    it["_hit_term"] = t
                all_items.extend(items)

            # ë””ë“€í”„(ë²•ë ¹ID/ë²•ë ¹ì¼ë ¨ë²ˆí˜¸/í˜„í–‰ì—°í˜ì½”ë“œ ë“± í›„ë³´ë¡œ)
            results = _dedupe_by(all_items, ["ë²•ë ¹ID", "ë²•ë ¹ì¼ë ¨ë²ˆí˜¸", "í˜„í–‰ì—°í˜ì½”ë“œ", "ë²•ë ¹ìƒì„¸ë§í¬"])

        else:
            if not query_direct.strip():
                st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                st.stop()
            payload = cached_search(query=query_direct.strip(), search_scope=search_scope, page=1, display=display, sort=sort)
            _, items = parse_search_results(payload)
            results = items

        if advanced:
            st.subheader("ì›ë³¸ ê²€ìƒ‰ JSON(ë””ë²„ê·¸)")
            st.json(payload if not mode.startswith("AI") else {"items_count": len(results), "sample": results[:3]})

        if not results:
            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ë³´ê¸° ì¢‹ê²Œ í‘œì¤€ ì»¬ëŸ¼ ë½‘ê¸° (ê°€ì´ë“œì— ìˆëŠ” í•„ë“œë“¤ ì¤‘ì‹¬) :contentReference[oaicite:6]{index=6}
        view_rows = []
        for i, it in enumerate(results, start=1):
            view_rows.append({
                "No": i,
                "ë²•ë ¹ëª…": it.get("ë²•ë ¹ëª…í•œê¸€") or it.get("ë²•ë ¹ëª…_í•œê¸€") or it.get("ë²•ë ¹ëª…") or it.get("ë²•ë ¹ì•½ì¹­ëª…") or "",
                "ì‹œí–‰ì¼ì": it.get("ì‹œí–‰ì¼ì") or "",
                "ê³µí¬ì¼ì": it.get("ê³µí¬ì¼ì") or "",
                "ì†Œê´€ë¶€ì²˜": it.get("ì†Œê´€ë¶€ì²˜ëª…") or it.get("ì†Œê´€ë¶€ì²˜") or "",
                "ë²•ë ¹ID": it.get("ë²•ë ¹ID") or "",
                "ë²•ë ¹ì¼ë ¨ë²ˆí˜¸": it.get("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸") or it.get("MST") or "",
                "_hit": it.get("_hit_term", ""),
            })

        st.subheader("ê²€ìƒ‰ ê²°ê³¼")
        st.dataframe(view_rows, use_container_width=True, hide_index=True)

        # ì„ íƒ
        options = [
            f"{r['No']:>02}. {r['ë²•ë ¹ëª…']} (ì‹œí–‰ {r['ì‹œí–‰ì¼ì']})"
            for r in view_rows
        ]
        pick = st.selectbox("ë²•ë ¹ ì„ íƒ", options=options, index=0)
        pick_no = int(pick.split(".")[0].strip())
        picked = view_rows[pick_no - 1]

        law_id = str(picked.get("ë²•ë ¹ID") or "").strip() or None
        mst = str(picked.get("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸") or "").strip() or None

        st.divider()
        st.subheader("ë²•ë ¹ ë³¸ë¬¸/ì¡°ë¬¸")
        detail = cached_detail(law_id=law_id, mst=mst)

        if advanced:
            st.subheader("ì›ë³¸ ìƒì„¸ JSON(ë””ë²„ê·¸)")
            st.json(detail)

        meta = pick_law_meta(detail)
        if meta:
            st.markdown("#### ë©”íƒ€")
            st.write(meta)

        articles = extract_articles_from_detail(detail)
        if not articles:
            st.warning("ìƒì„¸ JSONì—ì„œ 'ì¡°ë¬¸ë‚´ìš©'ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë””ë²„ê·¸ JSONì„ ì¼œê³  êµ¬ì¡°ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.)")
            st.stop()

        # í•˜ì´ë¼ì´íŠ¸ ê¸°ì¤€ ë‹¨ì–´
        if mode.startswith("AI"):
            hl_terms = _extract_terms_from_situation(situation, max_terms=12)
            hl_terms += [picked.get("_hit", "")] if picked.get("_hit") else []
            hl_terms = [t for t in hl_terms if t]
        else:
            hl_terms = [query_direct.strip()]

        # ì¡°ë¬¸ í•„í„°
        filter_word = st.text_input("ì¡°ë¬¸ í•„í„°(ì„ íƒ: ì´ ë‹¨ì–´ê°€ í¬í•¨ëœ ì¡°ë¬¸ë§Œ ë³´ê¸°)", value="")
        shown = 0

        for a in articles:
            title = a.get("ì¡°ë¬¸ì œëª©") or ""
            no = a.get("ì¡°ë¬¸ë²ˆí˜¸")
            no2 = a.get("ì¡°ë¬¸ê°€ì§€ë²ˆí˜¸")
            label = "ì¡°ë¬¸"
            if no is not None:
                label = f"ì œ{no}ì¡°" if _safe_int(no) is not None else f"{no}"
                if no2 and str(no2) not in ("0", "", "None"):
                    label += f"ì˜{no2}"

            body = a.get("ì¡°ë¬¸ë‚´ìš©") or ""
            plain = _normalize_space(re.sub(r"<[^>]+>", " ", str(body)))  # í˜¹ì‹œ HTMLì´ ì„ì´ë©´ ì œê±°

            if filter_word and (filter_word not in plain) and (filter_word not in title):
                continue

            # í•˜ì´ë¼ì´íŠ¸
            md = _highlight(plain, hl_terms)

            with st.expander(f"{label} {title}".strip(), expanded=False):
                st.markdown(md)

            shown += 1
            if shown >= 80:
                st.info("ì¡°ë¬¸ì´ ë§ì•„ 80ê°œê¹Œì§€ë§Œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. (í•„í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”)")
                break

        st.caption("ëª©ë¡ API: lawSearch.do?target=law :contentReference[oaicite:7]{index=7}  |  ë³¸ë¬¸ API: lawService.do?target=law :contentReference[oaicite:8]{index=8}")

    except requests.HTTPError as e:
        st.error(f"HTTP ì˜¤ë¥˜: {e}")
    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")
